{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Config File :  <class 'dict'>\n",
      "Keys in Config File :  dict_keys(['sql_steps'])\n",
      "The Values of sql_steps is a list:  True\n",
      "Value of Key 'sql_steps' :  [{'sql_num': '1', 'action': 'read|publish', 'sql': \"select original_claim_num_cd, parent_claim_num_cd, child_claim_num_cd, claim_ordinal_value_num from ods_vw_service.mv_consume_claim_ln_reprocess_assn where claim_ln_reprocess_tp_cd = 'RORGCL' union select original_claim_num_cd, parent_claim_num_cd, child_claim_num_cd, claim_ordinal_value_num from ods_vw_service.mv_consume_claim_ln_reprocess_assn where claim_ln_reprocess_tp_cd != 'RORGCL' and parent_claim_num_cd != child_claim_num_cd\", 'temp_view': 'consume_claim_ln_reprocess_assn', 'next_sql': '2'}, {'sql_num': '2', 'action': 'write|parquet', 'sql': 'select * from consume_claim_ln_reprocess_assn', 'location': 'sandbox/claims_reprocess/06-07/claim_ln_repr_ordinal_2/', 'mode': 'overwrite', 'next_sql': '3'}, {'sql_num': '3', 'action': 'read|parquet', 'location': 'sandbox/claims_reprocess/06-07/claim_ln_repr_ordinal_2/', 'temp_view': 'claim_ln_repr_ordinal', 'next_sql': '4'}, {'sql_num': '4', 'action': 'write|publish', 'temp_view': 'claim_ln_repr_ordinal', 'schema_name': 'ods_claim', 'table_name': 'claim_header_version_step_new_1', 'truncate': 0, 'next_sql': '5'}, {'sql_num': '5', 'action': 'read|publish', 'sql': 'select a.original_claim_num_cd as claim_num_cd , b.original_claim_num_cd as parent_claim_num_cd from ods_claim.claim_header_version_step_new_1 a, ods_claim.claim_header_version_step_new_1 b where a.original_claim_num_cd<>b.original_claim_num_cd and (a.original_claim_num_cd=b.parent_claim_num_cd or a.original_claim_num_cd=b.child_claim_num_cd )', 'temp_view': 'filtered_claims', 'next_sql': '6'}, {'sql_num': '6', 'action': 'write|parquet', 'sql': 'select * from filtered_claims', 'location': 'sandbox/claims_reprocess/06-07/claims_remove/', 'mode': 'overwrite', 'next_sql': '7'}, {'sql_num': '7', 'action': 'read|parquet', 'location': 'sandbox/claims_reprocess/06-07/claims_remove/', 'temp_view': 'claims_remove', 'cache': 'true', 'next_sql': '8'}, {'sql_num': '8', 'action': 'read|temp_view', 'sql': 'select step1.* from claim_ln_repr_ordinal step1 left join claims_remove r on step1.original_claim_num_cd = r.claim_num_cd where (step1.claim_ordinal_value_num !=0 and r.claim_num_cd = step1.original_claim_num_cd)', 'temp_view': 'claim_header_version_delete', 'next_sql': '9'}, {'sql_num': '9', 'action': 'read|temp_view', 'sql': 'select original_claim_num_cd, step2.parent_claim_num_cd, child_claim_num_cd, case when r.claim_num_cd is null then claim_ordinal_value_num else  (claim_ordinal_value_num + 1000) end as  claim_ordinal_value_num  from claim_header_version_delete step2 left join claims_remove r on claim_num_cd = original_claim_num_cd where step2.original_claim_num_cd = r.claim_num_cd', 'temp_view': 'claim_header_version_update1', 'next_sql': '10'}, {'sql_num': '10', 'action': 'read|temp_view', 'sql': 'select case when step3.original_claim_num_cd != r.claim_num_cd then original_claim_num_cd else r.parent_claim_num_cd end as original_claim_num_cd, step3.parent_claim_num_cd, child_claim_num_cd,   claim_ordinal_value_num  from claim_header_version_update1 step3 left join claims_remove r on claim_num_cd = original_claim_num_cd where step3.original_claim_num_cd = r.claim_num_cd', 'temp_view': 'claim_header_version_update2', 'next_sql': '11'}, {'sql_num': '11', 'action': 'read|temp_view', 'sql': 'select original_claim_num_cd, parent_claim_num_cd, child_claim_num_cd, claim_ordinal_value_num as claim_line_ordinal_value_num, claim_ordinal_value_num_final-1 as claim_header_version from ( select *, row_number() over(partition by original_claim_num_cd order by claim_ordinal_value_num) as claim_ordinal_value_num_final from claim_header_version_update2)a', 'temp_view': 'claim_header_version_step5', 'next_sql': '12'}, {'sql_num': '12', 'action': 'write|delta', 'sql': 'select * from claim_header_version_step5', 'schema_name': 'ods_ref_claim', 'table_name': 'claim_header_version_2', 'next_sql': '13'}, {'sql_num': '13', 'action': 'write|publish', 'temp_view': 'claim_header_version_step5', 'schema_name': 'ods_claim', 'table_name': 'claim_header_version_final_1', 'truncate': 0, 'next_sql': '14'}]\n",
      "Datatype <class 'dict'> Matched\n",
      "\n",
      "All Keys Found in Block  0\n",
      "sql_num exists:  1  Type:  <class 'str'>\n",
      "Type of sql_num typecasted to:  <class 'int'>\n",
      "Valid action read exists in Block  0 . \n",
      "Error : Invalid Format- publish exists in Block  0 . \n",
      "\n",
      "All Keys Found in Block  1\n",
      "sql_num exists:  2  Type:  <class 'str'>\n",
      "Type of sql_num typecasted to:  <class 'int'>\n",
      "Valid action write exists in Block  1 . \n",
      "Valid Format parquet exists in Block  1 . \n",
      "\n",
      "All Keys Found in Block  2\n",
      "sql_num exists:  3  Type:  <class 'str'>\n",
      "Type of sql_num typecasted to:  <class 'int'>\n",
      "Valid action read exists in Block  2 . \n",
      "Valid Format parquet exists in Block  2 . \n",
      "\n",
      "All Keys Found in Block  3\n",
      "sql_num exists:  4  Type:  <class 'str'>\n",
      "Type of sql_num typecasted to:  <class 'int'>\n",
      "Valid action write exists in Block  3 . \n",
      "Error : Invalid Format- publish exists in Block  3 . \n",
      "\n",
      "All Keys Found in Block  4\n",
      "sql_num exists:  5  Type:  <class 'str'>\n",
      "Type of sql_num typecasted to:  <class 'int'>\n",
      "Valid action read exists in Block  4 . \n",
      "Error : Invalid Format- publish exists in Block  4 . \n",
      "\n",
      "All Keys Found in Block  5\n",
      "sql_num exists:  6  Type:  <class 'str'>\n",
      "Type of sql_num typecasted to:  <class 'int'>\n",
      "Valid action write exists in Block  5 . \n",
      "Valid Format parquet exists in Block  5 . \n",
      "\n",
      "All Keys Found in Block  6\n",
      "sql_num exists:  7  Type:  <class 'str'>\n",
      "Type of sql_num typecasted to:  <class 'int'>\n",
      "Valid action read exists in Block  6 . \n",
      "Valid Format parquet exists in Block  6 . \n",
      "\n",
      "All Keys Found in Block  7\n",
      "sql_num exists:  8  Type:  <class 'str'>\n",
      "Type of sql_num typecasted to:  <class 'int'>\n",
      "Valid action read exists in Block  7 . \n",
      "Error : Invalid Format- temp_view exists in Block  7 . \n",
      "\n",
      "All Keys Found in Block  8\n",
      "sql_num exists:  9  Type:  <class 'str'>\n",
      "Type of sql_num typecasted to:  <class 'int'>\n",
      "Valid action read exists in Block  8 . \n",
      "Error : Invalid Format- temp_view exists in Block  8 . \n",
      "\n",
      "All Keys Found in Block  9\n",
      "sql_num exists:  10  Type:  <class 'str'>\n",
      "Type of sql_num typecasted to:  <class 'int'>\n",
      "Valid action read exists in Block  9 . \n",
      "Error : Invalid Format- temp_view exists in Block  9 . \n",
      "\n",
      "All Keys Found in Block  10\n",
      "sql_num exists:  11  Type:  <class 'str'>\n",
      "Type of sql_num typecasted to:  <class 'int'>\n",
      "Valid action read exists in Block  10 . \n",
      "Error : Invalid Format- temp_view exists in Block  10 . \n",
      "\n",
      "All Keys Found in Block  11\n",
      "sql_num exists:  12  Type:  <class 'str'>\n",
      "Type of sql_num typecasted to:  <class 'int'>\n",
      "Valid action write exists in Block  11 . \n",
      "Error : Invalid Format- delta exists in Block  11 . \n",
      "\n",
      "All Keys Found in Block  12\n",
      "sql_num exists:  13  Type:  <class 'str'>\n",
      "Type of sql_num typecasted to:  <class 'int'>\n",
      "Valid action write exists in Block  12 . \n",
      "Error : Invalid Format- publish exists in Block  12 . \n"
     ]
    }
   ],
   "source": [
    "#Importing the required libraries and opening the JSON file\n",
    "import json\n",
    "import pandas as pd\n",
    "try:\n",
    "    with open('C:/Users/dvive/Downloads/Project/Data/config.json', \"r\") as json_file:\n",
    "        config_file = json_file.read()\n",
    "except FileNotFoundError as error:\n",
    "    print(\"File not found:\", error)\n",
    "except Exception as error:\n",
    "    print(\"Error opening file:\", error)\n",
    "\n",
    "#Converting the JSON file to a dictionary\n",
    "try:\n",
    "    config_file = json.loads(config_file) \n",
    "except json.JSONDecodeError as error:\n",
    "    print(\"Error decoding JSON:\", error)\n",
    "except Exception as error:\n",
    "    print(\"Error loading config file:\", error)\n",
    "    \n",
    "#Saving the Primary Key of the JSON file\n",
    "primary_key = \"sql_steps\"\n",
    "\n",
    "#Printing the details of the JSON file\n",
    "print(\"Type of Config File : \",type(config_file))\n",
    "print(\"Keys in Config File : \",config_file.keys())\n",
    "print(\"The Values of sql_steps is a list: \",isinstance(config_file[\"sql_steps\"],list))\n",
    "print(\"Value of Key 'sql_steps' : \",config_file['sql_steps'])\n",
    "\n",
    "#Function to check if the key exists in the JSON file\n",
    "config_file.setdefault(primary_key,\"Key Not found\") \n",
    "#Function to check if the datatype of the variable matches the desired datatype \n",
    "def validate_datatype(var,datatype):\n",
    "    if type(var) == datatype:\n",
    "        return f\"Datatype {datatype} Matched\"\n",
    "    else:\n",
    "        return f\"Datatype {datatype} not Matched\"\n",
    "    \n",
    "#Calling the function to check the datatype of the variable    \n",
    "print(validate_datatype(config_file,dict))\n",
    "\n",
    "#Function to Validate Data in the config JSON file\n",
    "def check_keys(*args):\n",
    "    for i in range(len(config_file[primary_key])):\n",
    "        try:\n",
    "            #Saving repeating logic to variables\n",
    "            sql_num_value = config_file[primary_key][i].get(args[0])\n",
    "            action_value = config_file[primary_key][i].get(args[1])\n",
    "            action_parsed_value = config_file[primary_key][i].get(args[1]).split(\"|\")[0]\n",
    "            action_format_value = config_file[primary_key][i].get(args[1]).split(\"|\")[1]\n",
    "            next_sql_value = config_file[primary_key][i].get(args[2])\n",
    "            print()\n",
    "            #If condition to check if the keys exist in the config JSON file and converting sql_num to int\n",
    "            if sql_num_value is not None and action_value is not None and next_sql_value is not None:\n",
    "                print(\"All Keys Found in Block \",i)\n",
    "                if isinstance(sql_num_value,int)==False:\n",
    "                    print(\"sql_num exists: \",sql_num_value,\" Type: \",type(sql_num_value))\n",
    "                    #Typecast sql_num_value to int and save changes to config_file\n",
    "                    config_file[primary_key][i][args[0]] = int(sql_num_value)\n",
    "                    sql_num_value = int(sql_num_value)\n",
    "                    print(\"Type of sql_num typecasted to: \",type(sql_num_value))\n",
    "            else:\n",
    "                print(\"All Keys not Found in Block \",i)\n",
    "                if  sql_num_value is None:\n",
    "                    print(\"sql_num not Found in Block \",i)\n",
    "                if  action_value is None:\n",
    "                    print(\"action not Found in Block \",i)\n",
    "                if  next_sql_value is None:\n",
    "                    print(\"next_sql not Found in Block \",i)\n",
    "            #If condition to check if the action is read or write\n",
    "            if action_parsed_value not in (\"read\", \"write\"):\n",
    "                print(\"Error : Invalid action-\",action_parsed_value,\"exists in Block \",i,\". \")\n",
    "            else:\n",
    "                print(\"Valid action\",action_parsed_value,\"exists in Block \",i,\". \")   \n",
    "            #If condition to check if the action is read and the format is database or parquet\n",
    "            if action_parsed_value==\"read\" and action_format_value not in (\"database\",\"parquet\"):\n",
    "                        print(\"Error : Invalid Format-\",action_format_value,\"exists in Block \",i,\". \")\n",
    "            if action_parsed_value==\"read\" and action_format_value in (\"database\",\"parquet\"):\n",
    "                print(\"Valid Format\",action_format_value,\"exists in Block \",i,\". \")\n",
    "            #If condition to check if the action is write and the format is parquet\n",
    "            if action_parsed_value==\"write\" and action_format_value not in (\"parquet\"):\n",
    "                print(\"Error : Invalid Format-\",action_format_value,\"exists in Block \",i,\". \")   \n",
    "            if action_parsed_value==\"write\" and action_format_value in (\"parquet\"):\n",
    "                print(\"Valid Format\",action_format_value,\"exists in Block \",i,\". \")\n",
    "        except Exception as error:\n",
    "            print(f\"{error}\")\n",
    "\n",
    "\n",
    "\n",
    "#Calling the function to check the keys in the config JSON file    \n",
    "check_keys(\"sql_num\",\"action\",\"next_sql\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "def read_csv_file(location):\n",
    "    data = []\n",
    "    with open(location, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        for row in reader:\n",
    "            data.append(dict(zip(header, row)))\n",
    "    return data\n",
    "\n",
    "def write_csv_file(location, data):\n",
    "    with open(location, 'w') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def read_parquet_file(location):\n",
    "    data = []\n",
    "    with open(location, 'r') as f:\n",
    "        reader = parquet.reader(f)\n",
    "    return data\n",
    "\n",
    "def write_parquet_file(location):\n",
    "     with open(location, 'w') as f:\n",
    "        writer = parquet.DictWriter(f, fieldnames=data[0].keys())\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def read_database(location):\n",
    "    data = []\n",
    "    with open(location, 'r') as f:\n",
    "        reader = sql.reader(f)\n",
    "        header = next(reader)\n",
    "        for row in reader:\n",
    "            data.append(dict(zip(header, row)))\n",
    "    return data\n",
    "\n",
    "def write_database(location, data):\n",
    "    with open(location, 'w') as f:\n",
    "        writer = sql.DictWriter(f, fieldnames=data[0].keys())\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def read_file(action, format):\n",
    "    if action=='read':\n",
    "        if format=='csv':\n",
    "            read_csv_file(i[\"location\"])\n",
    "        if format=='parquet': \n",
    "            read_parquet_file(i[\"location\"])\n",
    "        if format=='database':\n",
    "            read_database(i[\"location\"])\n",
    "\n",
    "    if action=='write':\n",
    "        if format=='csv':\n",
    "            write_csv_file(i[\"location\"])\n",
    "        if format=='parquet': \n",
    "            write_parquet_file(i[\"location\"])\n",
    "        if format=='database':\n",
    "            write_database(i[\"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sandbox/claims_reprocess/06-07/claim_ln_repr_ordinal_2/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m config_file[\u001b[39m\"\u001b[39m\u001b[39msql_steps\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m----> 2\u001b[0m     read_file(i[\u001b[39m\"\u001b[39;49m\u001b[39maction\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m|\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m0\u001b[39;49m],i[\u001b[39m\"\u001b[39;49m\u001b[39maction\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m|\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39;49m])\n",
      "Cell \u001b[1;32mIn[27], line 62\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(action, format)\u001b[0m\n\u001b[0;32m     60\u001b[0m     write_csv_file(i[\u001b[39m\"\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mparquet\u001b[39m\u001b[39m'\u001b[39m: \n\u001b[1;32m---> 62\u001b[0m     write_parquet_file(i[\u001b[39m\"\u001b[39;49m\u001b[39mlocation\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdatabase\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     64\u001b[0m     write_database(i[\u001b[39m\"\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[27], line 26\u001b[0m, in \u001b[0;36mwrite_parquet_file\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite_parquet_file\u001b[39m(location):\n\u001b[1;32m---> 26\u001b[0m      \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(location, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     27\u001b[0m         writer \u001b[39m=\u001b[39m parquet\u001b[39m.\u001b[39mDictWriter(f, fieldnames\u001b[39m=\u001b[39mdata[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m     28\u001b[0m         writer\u001b[39m.\u001b[39mwriteheader()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sandbox/claims_reprocess/06-07/claim_ln_repr_ordinal_2/'"
     ]
    }
   ],
   "source": [
    "for i in config_file[\"sql_steps\"]:\n",
    "    read_file(i[\"action\"].split(\"|\")[0],i[\"action\"].split(\"|\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
